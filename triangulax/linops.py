# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_linear_operators.ipynb.

# %% auto #0
__all__ = ['compute_cotan_laplace', 'cotan_laplace_sparse', 'compute_gradient_2d', 'compute_gradient_3d', 'gradient_sparse_2d',
           'gradient_sparse_3d', 'reshape_face_gradient', 'scipy_to_bcoo', 'diag_jsparse', 'linear_op_to_sparse']

# %% ../nbs/07_linear_operators.ipynb #d159edd4-4456-41f8-b520-8b1b69219c67
import numpy as np
import igl

# %% ../nbs/07_linear_operators.ipynb #9f1cb15c-86cd-4e64-8f21-d4726216cd2f
import jax
import jax.numpy as jnp
import jax.experimental.sparse as jsparse

import lineax

import functools

# %% ../nbs/07_linear_operators.ipynb #723a50d1-f5c2-435c-9026-39b6067f426d
from jaxtyping import Float 

# %% ../nbs/07_linear_operators.ipynb #cef3ff0a
from . import trigonometry as trig
from . import mesh as msh
from . import adjacency as adj
from . import geometry as geom

# %% ../nbs/07_linear_operators.ipynb #66b2e04f
def compute_cotan_laplace(vertices: Float[jax.Array, "n_vertices dim"], hemesh: msh.HeMesh,
                          vertex_field: Float[jax.Array, "n_vertices ..."]
                          ) -> Float[jax.Array, "n_vertices ..."]:
    """
    Compute cotangent laplacian of a per-vertex field (natural boundary conditions).
    """
    w_edge = geom.get_cotan_weights_per_egde(vertices, hemesh)
    diff = vertex_field[hemesh.dest] - vertex_field[hemesh.orig]
    return -adj.sum_he_to_vertex_incoming(hemesh, (w_edge*diff.T).T)

# %% ../nbs/07_linear_operators.ipynb #b9c6f5cc
def cotan_laplace_sparse(vertices: Float[jax.Array, "n_vertices dim"], hemesh: msh.HeMesh
                         ) -> jsparse.BCOO:
    """Assemble cotangent Laplacian as a sparse matrix (BCOO)."""

    w_edge = geom.get_cotan_weights_per_egde(vertices, hemesh)
    unique = hemesh.is_unique

    i = hemesh.orig[unique]
    j = hemesh.dest[unique]
    w = w_edge[unique]

    rows = jnp.concatenate([i, j, i, j])
    cols = jnp.concatenate([j, i, i, j])
    data = jnp.concatenate([w, w, -w, -w])

    mat = jsparse.BCOO((data, jnp.stack([rows, cols], axis=1)),
                       shape=(hemesh.n_vertices, hemesh.n_vertices))
    return mat.sum_duplicates()

# %% ../nbs/07_linear_operators.ipynb #e2587568
def _fe_grad_phi_2d(vertices: Float[jax.Array, "n_vertices 2"], hemesh: msh.HeMesh,
                 ) -> Float[jax.Array, "n_faces 3 2"]:
    """Per-face gradients of the P1 hat functions (2D).

    For each face f=(i,j,k), returns an array grads[f, l, :] = ∇φ_l, with l=0,1,2
    corresponding to vertices (i,j,k). Degenerate faces get zero gradients.
    """
    faces = hemesh.faces
    v0, v1, v2 = (vertices[faces[:, 0]], vertices[faces[:, 1]], vertices[faces[:, 2]])

    area2 = jnp.cross(v1 - v0, v2 - v0)[:, None]
    mask = jnp.abs(area2) > 1e-12
    grad_phi0 = jnp.where(mask, trig.get_perp_2d(v1 - v2)/area2, 0)
    grad_phi1 = jnp.where(mask, trig.get_perp_2d(v2 - v0)/area2, 0)
    grad_phi2 = jnp.where(mask, trig.get_perp_2d(v0 - v1)/area2, 0)

    return jnp.stack([grad_phi0, grad_phi1, grad_phi2], axis=1)


def _fe_grad_phi_3d(vertices: Float[jax.Array, "n_vertices 3"], hemesh: msh.HeMesh,
                 ) -> Float[jax.Array, "n_faces 3 3"]:
    """Per-face gradients of the P1 hat functions (3D).

    For each face f=(i,j,k), returns grads[f, l, :] = ∇φ_l in R^3, l=0,1,2.
    Degenerate faces get zero gradients.
    """
    faces = hemesh.faces
    v0, v1, v2 = (vertices[faces[:, 0]], vertices[faces[:, 1]], vertices[faces[:, 2]])

    n = jnp.cross(v1 - v0, v2 - v0)
    area2 = jnp.linalg.norm(n, axis=-1)[:, None]**2
    mask = area2 > 1e-12
    
    mask = jnp.abs(area2) > 1e-12
    grad_phi0 = jnp.where(mask, jnp.cross(v1 - v2, n)/area2, 0)
    grad_phi1 = jnp.where(mask, jnp.cross(v2 - v0, n)/area2, 0)
    grad_phi2 = jnp.where(mask, jnp.cross(v0 - v1, n)/area2, 0)

    return jnp.stack([grad_phi0, grad_phi1, grad_phi2], axis=1)


def compute_gradient_2d(vertices: Float[jax.Array, "n_vertices 2"], hemesh: msh.HeMesh,
                        vertex_field: Float[jax.Array, "n_vertices ..."]
                        ) -> Float[jax.Array, "n_faces 2 ..."]:
    """Compute the linear finite-element gradient (constant per face)."""
    faces = hemesh.faces
    grads = _fe_grad_phi_2d(vertices, hemesh)
    vals = vertex_field[faces]
    return jnp.einsum("fvd,fv...->fd...", grads, vals)


def compute_gradient_3d(vertices: Float[jax.Array, "n_vertices 3"], hemesh: msh.HeMesh,
                        vertex_field: Float[jax.Array, "n_vertices ..."]
                        ) -> Float[jax.Array, "n_faces 3 ..."]:
    """Compute the linear finite-element gradient (constant per face)."""
    faces = hemesh.faces
    grads = _fe_grad_phi_3d(vertices, hemesh)
    vals = vertex_field[faces]
    return jnp.einsum("fvd,fv...->fd...", grads, vals)

# %% ../nbs/07_linear_operators.ipynb #ebdd9818
def gradient_sparse_2d(vertices: Float[jax.Array, "n_vertices 2"], hemesh: msh.HeMesh,
                      ) -> jsparse.BCOO:
    """Assemble FE gradient in 2D as a sparse matrix (BCOO).

    Returns a matrix G with shape (2*n_faces, n_vertices) such that for a scalar
    per-vertex field u (n_vertices,), the per-face gradients are obtained via:
        g_flat = G @ u                    # (2*n_faces,)
        g = g_flat.reshape((2, n_faces)).T  # (n_faces, 2)

    This row layout matches libigl's `grad` operator convention (component blocks).
    """
    faces = hemesh.faces.astype(jnp.int32)
    n_faces = faces.shape[0]
    n_vertices = hemesh.n_vertices

    grads = _fe_grad_phi_2d(vertices, hemesh)  # (n_faces, 3, 2)
    # order contributions as (component c, corner k, face f)
    data = jnp.transpose(grads, (2, 1, 0)).reshape((-1,))  # (2*3*n_faces,)
    cols_kf = faces.T.reshape((-1,))  # (3*n_faces,), order (k,f)
    cols = jnp.tile(cols_kf, (2,))
    rows_f = jnp.arange(n_faces, dtype=jnp.int32)
    rows_kf = jnp.tile(rows_f, (3,))  # (3*n_faces,), order (k,f)
    rows = jnp.tile(rows_kf, (2,)) + jnp.repeat(jnp.arange(2, dtype=jnp.int32) * n_faces, 3 * n_faces)

    indices = jnp.stack([rows, cols], axis=1)
    return jsparse.BCOO((data, indices), shape=(2 * n_faces, n_vertices))


def gradient_sparse_3d(vertices: Float[jax.Array, "n_vertices 3"], hemesh: msh.HeMesh,
                      ) -> jsparse.BCOO:
    """Assemble FE gradient in 3D as a sparse matrix (BCOO).

    Returns a matrix G with shape (3*n_faces, n_vertices) such that for a scalar
    per-vertex field u (n_vertices,), the per-face gradients are obtained via:
        g_flat = G @ u                    # (3*n_faces,)
        g = g_flat.reshape((3, n_faces)).T  # (n_faces, 3)

    This row layout matches libigl's `grad` operator convention (component blocks).
    """
    faces = hemesh.faces.astype(jnp.int32)
    n_faces = faces.shape[0]
    n_vertices = hemesh.n_vertices

    grads = _fe_grad_phi_3d(vertices, hemesh)  # (n_faces, 3, 3)
    data = jnp.transpose(grads, (2, 1, 0)).reshape((-1,))  # (3*3*n_faces,)
    cols_kf = faces.T.reshape((-1,))  # (3*n_faces,)
    cols = jnp.tile(cols_kf, (3,))
    rows_f = jnp.arange(n_faces, dtype=jnp.int32)
    rows_kf = jnp.tile(rows_f, (3,))
    rows = jnp.tile(rows_kf, (3,)) + jnp.repeat(jnp.arange(3, dtype=jnp.int32) * n_faces, 3 * n_faces)

    indices = jnp.stack([rows, cols], axis=1)
    return jsparse.BCOO((data, indices), shape=(3 * n_faces, n_vertices))

# %% ../nbs/07_linear_operators.ipynb #86653c6e
def reshape_face_gradient(grad_flat: Float[jax.Array, "dim_n_faces ..."], n_faces: int, dim: int,
                          ) -> Float[jax.Array, "n_faces dim ..."]:
    """Reshape a flattened FE gradient into per-face vectors.

    This is meant to be used with `gradient_sparse_2d/3d` (and any similar operator that
    stacks components in blocks), where applying the sparse matrix yields an array of shape
    `(dim*n_faces, ...)` (for scalar/vector/tensor per-vertex fields).

    Parameters
    ----------
    grad_flat
        Output of `G @ u`, with shape `(dim*n_faces, ...)`.
    n_faces
        Number of mesh faces.
    dim
        Spatial dimension (2 or 3).

    Returns
    -------
    grad
        Reshaped gradient with shape `(n_faces, dim, ...)`, matching the output convention
        of `compute_gradient_2d/3d`.
    """
    if grad_flat.shape[0] != dim * n_faces:
        raise ValueError(f"Expected grad_flat.shape[0] == dim*n_faces = {dim*n_faces}, got {grad_flat.shape[0]}")
    grad = jnp.reshape(grad_flat, (dim, n_faces) + grad_flat.shape[1:])
    return jnp.swapaxes(grad, 0, 1)

# %% ../nbs/07_linear_operators.ipynb #16986532
def scipy_to_bcoo(A) -> jsparse.BCOO:
    """
    Convert a SciPy sparse matrix (CSC or CSR) to a JAX BCOO sparse matrix
    without converting to dense.

    Parameters
    ----------
    A : scipy.sparse.spmatrix
        Input sparse matrix (CSR or CSC recommended)

    Returns
    -------
    B : jax.experimental.sparse.BCOO
        Equivalent JAX sparse matrix
    """
    # Convert to COO
    Acoo = A.tocoo()

    # COO format gives us row, col, data arrays directly
    rows = jnp.array(Acoo.row, dtype=jnp.int32)
    cols = jnp.array(Acoo.col, dtype=jnp.int32)
    data = jnp.array(Acoo.data)
    return jsparse.BCOO((data, jnp.stack([rows, cols], axis=1)), shape=Acoo.shape)


def diag_jsparse(v : Float[jax.Array, " N"], k: int =0) -> jsparse.BCOO:
    """Construct a diagonal jax.sparse array. Plugin replacement for np.diag"""
    N  = v.shape[0] + jnp.abs(k)
    if k >=0:
        row_inds = jnp.arange(k, N, dtype=jnp.int32)
    else:
        row_inds = jnp.arange(0, N+k, dtype=jnp.int32)
    return jsparse.BCOO((v, jnp.stack([row_inds-k, row_inds,], axis=1)), shape=(N, N))

# %% ../nbs/07_linear_operators.ipynb #3d9366df
def linear_op_to_sparse(op: callable,
                        in_shape: tuple[int, ...],
                        out_shape: tuple[int, ...],
                        dtype: jnp.dtype | None = None,
                        chunk_size: int = 256,
                        tol: float = 0.0,
                        ) -> jsparse.BCOO:
    """Build a sparse matrix for a linear map using batched one-hot probes.

    Note: this function is general, but not necessarily very efficient for large matrix sizes.
    """
    if len(in_shape) != 1 or len(out_shape) != 1:
        raise ValueError("Only 1D input/output supported for now.")
    n_in = in_shape[0]
    n_out = out_shape[0]
    if dtype is None:
        dtype = jnp.result_type(op(jnp.zeros((n_in,))))

    data_list: list[np.ndarray] = []
    row_list: list[np.ndarray] = []
    col_list: list[np.ndarray] = []

    for start in range(0, n_in, chunk_size):
        end = min(start + chunk_size, n_in)
        idx = jnp.arange(start, end, dtype=jnp.int64)
        basis = jax.nn.one_hot(jnp.array(idx), n_in, dtype=dtype)
        cols = jax.vmap(op)(basis)  # (chunk, n_out)
        #cols = apply_op(basis)
        mask = jnp.abs(cols) > tol
        col_in_batch, row_out = jnp.nonzero(mask)
        if col_in_batch.size == 0:
            continue

        data_list.append(cols[col_in_batch, row_out])
        row_list.append(row_out)
        col_list.append(idx[col_in_batch])

    if len(data_list) == 0:
        return jsparse.empty((n_out, n_in)) 
    data = jnp.concatenate(data_list)
    rows = jnp.concatenate(row_list)
    cols = jnp.concatenate(col_list)
    indices = jnp.stack([jnp.array(rows, dtype=jnp.int32),
                            jnp.array(cols, dtype=jnp.int32)], axis=1)
    return jsparse.BCOO((data, indices), shape=(n_out, n_in))
