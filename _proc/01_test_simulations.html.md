---
title: Simulation test case - optimize mesh to make triangles equilateral
---



Let's start with a simple test case to see whether the technology developed so far actually works.
The goal is to move the vertices of triangulation so all triangle edge lengths are as close to some $\ell_0$ as possible. 
We specify a pseudo-energy $E=\sum_{ij} (|\mathbf{v}_i-\mathbf{v}_j| - \ell_0)^2 $, and then minimize it using the JAX-provided gradients w.r.t the vertex positions.

This defines the "forward pass" of our "dynamical" model. In a second step, we can optimize over the model parameters, like $\ell_0$, to make the dynamics return some desired shape, for examlpe.


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

::: {#d159edd4-4456-41f8-b520-8b1b69219c67 .cell}
``` {.python .cell-code}
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt

import copy

from tqdm.notebook import tqdm
```
:::


::: {#9f1cb15c-86cd-4e64-8f21-d4726216cd2f .cell execution_count=2}
``` {.python .cell-code}
import jax
import jax.numpy as jnp
```
:::


::: {#465aee96-fe0c-4994-97a5-d3ba628df4cf .cell execution_count=3}
``` {.python .cell-code}
jax.config.update("jax_enable_x64", True)
jax.config.update("jax_debug_nans", False)
jax.config.update('jax_log_compiles', False)
```
:::


::: {#723a50d1-f5c2-435c-9026-39b6067f426d .cell execution_count=4}
``` {.python .cell-code}
from jaxtyping import Float, Int, Bool, PyTree 
from typing import Any, Iterable, List, Dict, Tuple, NamedTuple
from enum import IntEnum

import dataclasses

import functools
```
:::


::: {#8832360d-b1aa-407f-9687-fba83222a08f .cell execution_count=12}
``` {.python .cell-code}
# import previously defined modules
from triangulax import mesh as msh
from triangulax.mesh import TriMesh, HeMesh, GeomMesh
```
:::


#### JAX-compatible scientific computing libraries - useful in future steps
```
import diffrax # ODE solvers
import lineax # linear solvers
import optimistix # optimisation (scientific-computing style, e.g. conjugate gradient)
import optax # optimisation (ML-style, e.g. ADAM)
```

### Forward pass - minimize energy

::: {#fdc42696-0274-41d8-bea8-41ea188b2430 .cell execution_count=23}
``` {.python .cell-code}
import equinox as eqx

# equinox has automated "filtering" of JAX-transforms. So we can work with objects which are not just pytrees of arrays
# (like neural networks) and appy jit, vmap etc
```
:::


::: {#a1466484-c167-4d4e-97fc-3f622728062d .cell execution_count=24}
``` {.python .cell-code}
# load example mesh
mesh = TriMesh.read_obj("test_meshes/disk.obj")
hemesh = HeMesh.from_triangles(mesh.vertices.shape[0], mesh.faces)
geommesh = GeomMesh(*hemesh.n_items, vertices=mesh.vertices)

hemesh
```

::: {.cell-output .cell-output-stderr}
```
Warning: readOBJ() ignored non-comment line 3:
  o flat_tri_ecmc
```
:::

::: {.cell-output .cell-output-display execution_count=24}
```
HeMesh(N_V=131, N_HE=708, N_F=224)
```
:::
:::


::: {#82e699dd-6ad6-4fcd-aa23-bdfdb170fe0a .cell execution_count=25}
``` {.python .cell-code}
fig = plt.figure(figsize=(4, 4))
plt.triplot(*geommesh.vertices.T, mesh.faces)
plt.axis("equal");
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-9-output-1.png){}
:::
:::


::: {#ebcbb331-3b05-4656-b2aa-4224d4b17e32 .cell execution_count=26}
``` {.python .cell-code}
lengths = jnp.linalg.norm(geommesh.vertices[hemesh.orig]-geommesh.vertices[hemesh.dest], axis=1)
tri_areas = -jax.vmap(msh.get_polygon_area)(geommesh.vertices[hemesh.faces])

lengths.mean(), tri_areas.mean()
```

::: {.cell-output .cell-output-display execution_count=26}
```
(Array(0.18296366, dtype=float64), Array(0.01391806, dtype=float64))
```
:::
:::


We write the energy_function using a geommesh as an argument. This is overkill for present purposes ince only the vertex positions matter, but is useful to test the machinery. For more complicated simulations, we will want to use GeomMesh as a wrapper for the various arrays.

::: {#6c1f897a-571b-48ec-9f06-5f99dba2ad93 .cell execution_count=28}
``` {.python .cell-code}
@jax.jit
def energy_function(geommesh: GeomMesh, hemesh: HeMesh, ell_0: float=1):
    edge_lengths = jnp.linalg.norm(geommesh.vertices[hemesh.orig]-geommesh.vertices[hemesh.dest], axis=1)
    edge_energy = jnp.mean((edge_lengths/ell_0-1)**2) # this way, term is "auto-normalized"
    # let's add a term for the triangle areas
    a_0 = (np.sqrt(3)/4) * ell_0**2 # area of equilateral triangle
    tri_area = -jax.vmap(msh.get_polygon_area)(geommesh.vertices[hemesh.faces])
    area_energy = jnp.mean((tri_area/a_0-1)**2)
    #jax.debug.print("E_l: {E_l}, E_a: {E_a}",  E_l=edge_energy, E_a=area_energy)
    # this is how you can print inside a JITed-function
    return edge_energy + area_energy
```
:::


::: {#95cf9ba7-78da-40df-b2b2-aeeeadb2250c .cell execution_count=29}
``` {.python .cell-code}
energy_function(geommesh, hemesh)
```

::: {.cell-output .cell-output-display execution_count=29}
```
Array(1.60519339, dtype=float64)
```
:::
:::


::: {#61e3eff9-afeb-4a34-8bcf-280494ac40c3 .cell execution_count=30}
``` {.python .cell-code}
# using functools.partial, we can fill in some of our function's args, like so:

functools.partial(energy_function, hemesh=hemesh, ell_0=1)(geommesh)
```

::: {.cell-output .cell-output-display execution_count=30}
```
Array(1.60519339, dtype=float64)
```
:::
:::


::: {#918fbcd1-512c-4f6c-9473-7f86c64b332e .cell scrolled='true' execution_count=31}
``` {.python .cell-code}
val, grad = jax.value_and_grad(energy_function)(geommesh, hemesh) # computing value and grad works

val, grad, grad.vertices.shape # the gradient is another GeomMesh with the same structure
```

::: {.cell-output .cell-output-display execution_count=31}
```
(Array(1.60519339, dtype=float64),
 GeomMesh(D=2,N_V=131, N_HE=708, N_F=224),
 (131, 2))
```
:::
:::


::: {#498b6da3-da6f-45d0-b3b7-7b7a90575069 .cell execution_count=32}
``` {.python .cell-code}
connectivity_grad = jax.grad(energy_function, argnums=1, allow_int=True)(geommesh, hemesh)
# we can even compute the gradient w.r.t to the connectivity matrix. It is also a HeMesh
connectivity_grad, connectivity_grad.dest[0] # whatever that means
```

::: {.cell-output .cell-output-display execution_count=32}
```
(HeMesh(N_V=131, N_HE=708, N_F=224), np.void((b'',), dtype=[('float0', 'V')]))
```
:::
:::


#### Optimization run

::: {#42641806-1873-4d07-ba52-0a8374f31965 .cell execution_count=43}
``` {.python .cell-code}
# parameters of the energy
ell_0 = 0.5

# parameters of the "optimizer"
step_size = 0.05
N_steps = 10000

@jax.jit
def make_step(geommesh: GeomMesh, hemesh: HeMesh, ell_0: float = 1, step_size: float = 0.01):
    loss, grad = jax.value_and_grad(energy_function)(geommesh, hemesh, ell_0=ell_0)

    # update - can be streamlined with equinox
    updated_vertices = geommesh.vertices - step_size*grad.vertices
    geommesh_updated = dataclasses.replace(geommesh, vertices=updated_vertices)
    return geommesh_updated, hemesh, loss # explicitly return the hemesh - may need to be updated by flips!

# define inital condition
geommesh_optimized = copy.copy(geommesh)
hemesh_optimized = copy.copy(hemesh)

losses = []

for step in range(N_steps):
    geommesh_optimized, hemesh_optimized, loss = make_step(geommesh_optimized, hemesh_optimized,
                                                           ell_0=ell_0, step_size=step_size)
    losses.append(loss)

losses = np.array(losses)
```
:::


::: {#daab7173-68b8-4856-a6ee-316ab1768611 .cell execution_count=44}
``` {.python .cell-code}
fig = plt.figure(figsize=(4, 3))
plt.plot(losses)
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-17-output-1.png){}
:::
:::


::: {#c6a4ed26-9893-401d-bd9f-8b0ae861c416 .cell execution_count=45}
``` {.python .cell-code}
fig = plt.figure(figsize=(4, 4))
plt.triplot(*geommesh.vertices.T, hemesh.faces)
plt.triplot(*geommesh_optimized.vertices.T, hemesh_optimized.faces)
plt.axis("equal");
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-18-output-1.png){}
:::
:::


::: {#a6fcffe4-7f74-429b-9671-021b59a62e4a .cell execution_count=46}
``` {.python .cell-code}
lengths_optimized = jnp.linalg.norm(geommesh_optimized.vertices[hemesh_optimized.orig]
                                   -geommesh_optimized.vertices[hemesh_optimized.dest], axis=1)
jnp.abs(lengths_optimized-ell_0).mean(), lengths_optimized.mean()
```

::: {.cell-output .cell-output-display execution_count=46}
```
(Array(0.03342644, dtype=float64), Array(0.50208086, dtype=float64))
```
:::
:::


### Meta-training

Eventually, we aim to learn some dynamical rules for a tissue mechanics model that make the tissue carry out some desired behavior, like making a target shape.

As a toy example, let's take the above "dynamics" which minimizes the pseudo-energy to make all triangles equilateral. It depends on the parameter $\ell_0$. Relaxation of the pseudo-energy for some number of steps defines our "forward pass". Let's try to optimize $\ell_0$ so that the tissue, at the end of the energy relaxation, has some target size (of course, a contrived problem, since we know the solution from the start).

::: {#47b7030f-c6db-4541-82d0-edd1dedd3dea .cell execution_count=47}
``` {.python .cell-code}
# package the whole training process into a JITed function

@functools.partial(jax.jit, static_argnames=['N_steps'])
def relax_energy(initial_geommesh: GeomMesh, initial_hemesh: HeMesh, ell_0: float = 1,
                 step_size: float = 0.01, N_steps: int = 1):

    # define initial condition
    geommesh_optimized = copy.copy(initial_geommesh)
    hemesh_optimized = copy.copy(initial_hemesh)

    # use a jax.lax.fori_loop loop for training. Much faster JIT-compilation than a Python for loop.
    loss = 0
    init = (initial_geommesh, initial_hemesh, loss)
    loop_fun = lambda i, carry: make_step(carry[0], carry[1], ell_0=ell_0, step_size=step_size) 
    geommesh_optimized, hemesh_optimized, loss = jax.lax.fori_loop(0, N_steps, loop_fun, init, unroll=None)
    
    return (geommesh_optimized, hemesh_optimized), loss
```
:::


::: {#8c6b1121-f817-4341-9315-751de2436a79 .cell execution_count=49}
``` {.python .cell-code}
(geommesh_optimized, hemesh_optimized), losses = relax_energy(geommesh, hemesh, ell_0=0.5, step_size=0.05, N_steps=5000)
losses
```

::: {.cell-output .cell-output-display execution_count=49}
```
Array(0.04463656, dtype=float64)
```
:::
:::


::: {#7cd3217b-b9a0-40d0-9bee-13c0028c1f0b .cell execution_count=50}
``` {.python .cell-code}
fig = plt.figure(figsize=(4, 4))
plt.triplot(*geommesh.vertices.T, hemesh.faces)
plt.triplot(*geommesh_optimized.vertices.T, hemesh_optimized.faces)
plt.axis("equal");
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-22-output-1.png){}
:::
:::


::: {#fcec795e-e72b-4803-8154-7354d61362c7 .cell execution_count=52}
``` {.python .cell-code}

```

::: {.cell-output .cell-output-stdout}
```
28.9 ms ± 285 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```
:::
:::


#### Define Meta-training loss

Now we need to define our meta-training loss. In this case, it's just the deviation of the average edge length from the total.
Note how the meta-loss is _distinct_ from the pseudo-energy we minimize during the forward pass.

Let's use the `equinox` library to handle our problem, in anticipation of more complex ones down the line.

::: {#7d7790ad-a9e3-4947-8a2d-eca63aa611af .cell}
``` {.python .cell-code}
class RelaxationDynamics(eqx.Module): # we create a model wrapping our relaxational dynamics 
    ell_0: jax.Array
    step_size : float = eqx.field(static=True)
    N_steps : int = eqx.field(static=True)

    def __call__(self, initial_geommesh: GeomMesh, initial_hemesh: HeMesh) -> Tuple[GeomMesh, HeMesh]:
        init = (initial_geommesh, initial_hemesh, 0)
        loop_fun = lambda i, carry: make_step(carry[0], carry[1], ell_0=self.ell_0, step_size=self.step_size)
        geommesh_optimized, hemesh_optimized, loss = jax.lax.fori_loop(0, N_steps, loop_fun, init, unroll=None)
        return geommesh_optimized, hemesh_optimized
```
:::


::: {#af2694a6-0ad5-4204-8752-6ba0a13c8a40 .cell}
``` {.python .cell-code}
# define the meta-loss

def meta_loss(model: RelaxationDynamics, initial_geommesh: GeomMesh, initial_hemesh: HeMesh,  meta_ell0: float) -> float:
    geommesh_optimized, hemesh_optimized = model(initial_geommesh, initial_hemesh)
    lengths = jnp.linalg.norm(geommesh_optimized.vertices[hemesh_optimized.orig]
                              -geommesh_optimized.vertices[hemesh_optimized.dest], axis=-1)
    return jnp.mean((lengths/meta_ell0-1)**2)
```
:::


::: {#0c33f24a-4410-496d-abb9-d16613f4ae8b .cell execution_count=57}
``` {.python .cell-code}
# initialize the model, and test evaluating it

step_size = 0.01
N_steps = 20000

initial_ell0 = 0.4

meta_ell0 = 0.3

model_initial = RelaxationDynamics(ell_0=jnp.array([initial_ell0]), step_size=step_size, N_steps=N_steps)
model_initial(geommesh, hemesh), meta_loss(model_initial, geommesh, hemesh, meta_ell0=meta_ell0)
```

::: {.cell-output .cell-output-display execution_count=57}
```
((GeomMesh(D=2,N_V=131, N_HE=708, N_F=224),
  HeMesh(N_V=131, N_HE=708, N_F=224)),
 Array(0.11642016, dtype=float64))
```
:::
:::


::: {#fc4c4f6c-b3a4-49ef-b0a1-afbe3c4b7888 .cell execution_count=58}
``` {.python .cell-code}
# let's check that the model still does what we want - looks good!
geommesh_trained, hemesh_trained = model_initial(geommesh, hemesh)

fig = plt.figure(figsize=(4, 4))
plt.triplot(*geommesh.vertices.T, hemesh.faces)
plt.triplot(*geommesh_trained.vertices.T, hemesh_trained.faces)
plt.axis("equal");
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-27-output-1.png){}
:::
:::


#### Batching

To evaluate the loss, we want to average over a bunch of initial conditions. These are analogous to _batches_ in a normal ML problem.

::: {#f3a7bc45 .cell execution_count=60}
``` {.python .cell-code}
geommesh
```

::: {.cell-output .cell-output-display execution_count=60}
```
GeomMesh(D=2,N_V=131, N_HE=708, N_F=224)
```
:::
:::


::: {#a35a2157-3448-4a79-ae88-acf204948f89 .cell execution_count=63}
``` {.python .cell-code}
## Let us create a bunch of meshes with different initial positions and see if we can batch over them using vmap

key = jax.random.key(0)
sigma = 0.02

batch_geom = []
batch_he = []
for i in range(3):
    key, subkey = jax.random.split(key)
    random_noise = jax.random.normal(subkey, shape=geommesh.vertices.shape)
    batch_geom.append(dataclasses.replace(geommesh, vertices=geommesh.vertices+sigma*random_noise))
    batch_he.append(copy.copy(hemesh))

# we use a jax.tree.map to "push" the list axis into the underlying arrays.
batch_he_array = msh.tree_stack(batch_he)
batch_geom_array = msh.tree_stack(batch_geom)
batch_geom_array, batch_geom_array.vertices.shape
```

::: {.cell-output .cell-output-display execution_count=63}
```
(GeomMesh(D=2,N_V=131, N_HE=708, N_F=224), (3, 131, 2))
```
:::
:::


::: {#fd1b046f-a28e-4d20-a0a5-03a71e992e4e .cell execution_count=64}
``` {.python .cell-code}
# The result is a single mesh object with batch axes

batch_geom_array_out, batch_he_array_out = jax.vmap(model_initial)(batch_geom_array, batch_he_array) 
batch_geom_array_out, batch_geom_array_out.vertices.shape, batch_he_array_out.orig.shape
```

::: {.cell-output .cell-output-display execution_count=64}
```
(GeomMesh(D=2,N_V=131, N_HE=708, N_F=224), (3, 131, 2), (3, 708))
```
:::
:::


::: {#dbc09d0d-b15c-49f6-b7f1-c3cf714fc507 .cell execution_count=65}
``` {.python .cell-code}
# we can unpack things again into a list of meshes

batch_geom_out = msh.tree_unstack(batch_geom_array_out)
batch_he_out = msh.tree_unstack(batch_he_array_out)
```
:::


::: {#ab19b28b-7c70-4b30-ad73-e9de2eacb8e8 .cell execution_count=66}
``` {.python .cell-code}
# still works

i = 2
fig = plt.figure(figsize=(4, 4))
plt.triplot(*batch_geom[i].vertices.T, batch_he[i].faces)
plt.triplot(*batch_geom_out[i].vertices.T, batch_he_out[i].faces)
plt.axis("equal");
```

::: {.cell-output .cell-output-display}
![](01_test_simulations_files/figure-html/cell-32-output-1.png){}
:::
:::


::: {#8fb7bcca-beba-4816-8be8-e9e8f3c89c91 .cell execution_count=68}
``` {.python .cell-code}
# the batches are not identical, which is good.
not np.allclose(batch_geom_out[0].vertices, batch_geom_out[1].vertices)
```

::: {.cell-output .cell-output-display execution_count=68}
```
True
```
:::
:::


##### Compute the batched loss

::: {#3db6ad4d-59f7-4ef9-b634-4f6290ca80cb .cell execution_count=69}
``` {.python .cell-code}
# This is the right way to vmap the loss
jax.vmap(meta_loss, in_axes=(None, 0,0, None))(model_initial, batch_geom_array, batch_he_array, 0.8)
```

::: {.cell-output .cell-output-display execution_count=69}
```
Array([0.25692552, 0.25749475, 0.25687504], dtype=float64)
```
:::
:::


::: {#de3303c5-baa1-4b9c-ba68-28e2f311b45c .cell execution_count=70}
``` {.python .cell-code}
# check against non-vmapped version. pretty similar, floating point errors likely at origin of differences
[meta_loss(model_initial, batch_geom_out[i], batch_he_out[i], 0.8) for i in range(3)]
```

::: {.cell-output .cell-output-display execution_count=70}
```
[Array(0.24862364, dtype=float64),
 Array(0.24863551, dtype=float64),
 Array(0.24863234, dtype=float64)]
```
:::
:::


### Outer optimization

Now we are in a position to "optiomize" our model parameter `ell_0`. Based on [equinox CNN tutorial](https://docs.kidger.site/equinox/examples/mnist/#training).

::: {#5b0282a2-9cbd-4d35-9648-09695a64feef .cell execution_count=71}
``` {.python .cell-code}
def batched_meta_loss(model, batch_geom_array, batch_he_array, meta_ell0):
    return jnp.mean(jax.vmap(meta_loss, in_axes=(None, 0,0, None))(model, batch_geom_array, batch_he_array, meta_ell0))

batched_meta_loss_jit = jax.jit(batched_meta_loss)
```
:::


::: {#fa1e5bce-9a73-46ba-8fa7-9bcb5715ab9b .cell execution_count=73}
``` {.python .cell-code}
## Let's do a short profiling run - how much does JIT-compilation save? I guess a little!
```

::: {.cell-output .cell-output-stdout}
```
364 ms ± 2.93 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
:::
:::


::: {#4dc7d5b7-2658-492b-8b58-3baefac6b8db .cell execution_count=75}
``` {.python .cell-code}

```

::: {.cell-output .cell-output-stdout}
```
466 ms ± 6.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```
:::
:::


::: {#34be4913-0f58-402a-8a3b-dfde5c87dc17 .cell execution_count=76}
``` {.python .cell-code}
# hyper-parameters for the outer learning step.

BATCH_SIZE = 3
LEARNING_RATE = 1e-2
LEARNING_STEPS = 20
print_every = 2

step_size = 0.01
N_steps = 20000

META_ELL0 = 0.4
initial_ell0 = 0.2

model_initial = RelaxationDynamics(ell_0=jnp.array([initial_ell0]), step_size=step_size, N_steps=N_steps)
```
:::


::: {#f117bfab-9fba-413c-a3d5-3735d94ba486 .cell execution_count=82}
``` {.python .cell-code}
loss, grads = eqx.filter_jit(eqx.filter_value_and_grad(batched_meta_loss))(model_initial,
                                                                           batch_geom_array, batch_he_array, META_ELL0)
```
:::


::: {#2ffece5d-2274-4bc5-8106-a5f8a7e277cd .cell execution_count=83}
``` {.python .cell-code}
loss, grads, grads.ell_0
```

::: {.cell-output .cell-output-display execution_count=83}
```
(Array(0.24848878, dtype=float64),
 RelaxationDynamics(ell_0=f64[1], step_size=0.01, N_steps=20000),
 Array([-2.48070057], dtype=float64))
```
:::
:::


#### Forward and reverse mode autodiff

Since we are differentiation w.r.t. a small number of parameters (justy 1:  $\ell_0$), we can use forward mode automatic differentiation for increased efficiency. This may be the case more generally: if we want to learn "translationally invariant" models, where the parameters for all cells are equal, the parameter count we want to differentiate by may be small. Forward mode autodiff is also somewhat more "forgiving" when it comes to control flow.

See: https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html

::: {#b687e2fc-9866-4b55-867e-5812a10711ef .cell execution_count=84}
``` {.python .cell-code}
@eqx.filter_jit
def outer_optimizer_step(model: RelaxationDynamics,
                         batch_geom: GeomMesh, batch_he: HeMesh) -> Tuple[RelaxationDynamics, float]:
    
    # compute loss and grad on batch
    
    #loss, grads = eqx.filter_value_and_grad(batched_meta_loss)(model, batch_geom_array, batch_he_array, META_ELL0)
    #updates = jax.tree.map(lambda g: None if g is None else -LEARNING_RATE * g, grads)
    #model = eqx.apply_updates(model, updates)
    # grads is a PyTree with the same leaves as the trainable arrays of the model

    # same story, but using forward mode autodiff
    loss, grads = eqx.filter_jvp(lambda model: batched_meta_loss(model, batch_geom_array, batch_he_array, META_ELL0),
                                 primals=[model,], tangents=[model,])
    grads = grads/model.ell_0 # we used the current model values as a tangent vector, so we need to normalize
    model = dataclasses.replace(model, ell_0=model.ell_0-LEARNING_RATE*grads)
    
    return model, loss
```
:::


::: {#0c358a5d-3dbd-47de-b4cd-f8366b62a12c .cell execution_count=85}
``` {.python .cell-code}
model_stepped, loss = outer_optimizer_step(model_initial, batch_geom_array, batch_he_array)
```
:::


::: {#c98649a2-daeb-4202-88e0-511a5a2ab7af .cell execution_count=86}
``` {.python .cell-code}
loss, model_initial.ell_0, model_stepped.ell_0
```

::: {.cell-output .cell-output-display execution_count=86}
```
(Array(0.24848878, dtype=float64),
 Array([0.2], dtype=float64),
 Array([0.22480701], dtype=float64))
```
:::
:::


::: {#69a0b5b2-6e9d-466a-8e69-483669109251 .cell}
``` {.python .cell-code}
model = model_initial

for step in tqdm(range(LEARNING_STEPS)): # in the future, could also iterate over the initial conditions/batches
    model, loss = outer_optimizer_step(model, batch_geom_array, batch_he_array)
    if (step % print_every) == 0:
        print(f"Step: {step}, loss: {loss}, param: {model.ell_0}")

# 19s with forward mode vs 32s with reverse mode.
```

::: {.cell-output .cell-output-display}
```{=html}
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"0a5d2094234f405099488d3f8bb41705","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
```
:::

::: {.cell-output .cell-output-stdout}
```
Step: 0, loss: 0.24848877513396528, param: [0.22480701]
Step: 2, loss: 0.14704976288187782, param: [0.26531774]
Step: 4, loss: 0.08837946252729874, param: [0.29610925]
Step: 6, loss: 0.05458715535792681, param: [0.31944763]
Step: 8, loss: 0.03524215346058198, param: [0.33709312]
Step: 10, loss: 0.024176542728598434, param: [0.35045283]
Step: 12, loss: 0.017795094517620076, param: [0.36062556]
Step: 14, loss: 0.01405819184500635, param: [0.36843856]
Step: 16, loss: 0.0118282405357572, param: [0.37449757]
Step: 18, loss: 0.0104714494134906, param: [0.37924151]
```
:::
:::


Looks good -  the optimizer converges to the correct value of $\ell_0$.

###  Next steps

Success: we can solve this (stupid) toy problem. Our JAX-compatible infrastructure for vertex models seems to work, and we can autodiff through a simulation. Next steps: 

1. Toy simulations _with T1s_
2. More complex models - say, the area-perimeter vertex model
3. Play around with neural ODEs and neural optimizers more generally.



