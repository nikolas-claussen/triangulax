{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e9be26-9863-4a08-a379-8052d9e62f30",
   "metadata": {},
   "source": [
    "## Coding in JAX\n",
    "\n",
    "`triangulax` aims to create a triangulation datastructure compatible with the JAX library for automatic differentiation and numerical computing (see [JAX- the sharp bits](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html)). What does this mean in practice?\n",
    "\n",
    "1. Use `jnp` (=`jax.numpy`) instead of `numpy`.\n",
    "2. Use a _functional programming_ paradigm (pure functions, no side-effects). Avoid dynamically changing array shapes. For example, insead of in-place array modifications, use JAX's `x = x.at[idx].set(y)`.\n",
    "3. Use JAX idioms for [control flow](https://docs.jax.dev/en/latest/control-flow.html)\n",
    "4. _Register_ any new classes, so JAX knows how to handle them during gradient-computation and just-in-time compilation. See [here](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html#using-jax-jit-with-class-methods) and [here](https://docs.jax.dev/en/latest/custom_pytrees.html).\n",
    "\n",
    "To provide type signatures for all functions (what are the inputs? what do the array dimensions mean?), we use  [jaxtyping](https://docs.kidger.site/jaxtyping). Lateron, we will alsop use the `equinox` library, which adds a few useful tools to JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5028f5f-39cf-4dd9-9144-b513318cca8a",
   "metadata": {},
   "source": [
    "### PyTrees\n",
    "\n",
    "JAX supports not only arrays as inputs/outputs/intermediate variables, but also [pytrees](https://docs.jax.dev/en/latest/pytrees.html). Pytrees are nested structures (dicts, lists-of-lists, etc) whose leaves are \"elementary\" objects like arrays. Fortunately, our triangulation datastructure classes are already a lot like a pytree - it is a collection of arrays. For JAX to understand this, we need to register our classes as a [custom pytree node](https://docs.jax.dev/en/latest/custom_pytrees.html#pytrees-custom-pytree-nodes) via `jax.tree_util.register_dataclass`.\n",
    "\n",
    "Sidenote: Neural networks, in libraries like Flax or Equinox, are basically very similar. They are dataclass-like classes which hold all the arrays associated with a NN (the different weights, and maybe some parameters) with class methods like `__call__` specifying the forward pass through the NN. Equinox automatically registers your NN as a pytree by inheriting from the `equinox.Module` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb955a-0d29-40e2-a48b-63efa87f729c",
   "metadata": {},
   "source": [
    "### [Control flow](https://docs.jax.dev/en/latest/control-flow.html)\n",
    "\n",
    "For just-in-time compilation, JAX distinguishes two types of variables: dynamic and static. Control flow cannot depend on the _value_ of dynamic variables, only on their shape.\n",
    "\n",
    "Upshots:\n",
    "1. replace `if` with `jax.lax.cond` / `jnp.where` (full autodiff compatible), and `while` with `jax.lax.while_loop` (forward autodiff only).\n",
    "2. mark variables which are not going to change during simulation as static."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21491f",
   "metadata": {},
   "source": [
    "### Static array shapes\n",
    "\n",
    "JAX works best if the _shapes_ of arrays do not change during the computation. For this reason, we (first) focus on triangulations where the number of vertices does not change. \n",
    "Topological modifications (like edge flips) are nevertheless possible, as long as they do not change the number of mesh elements (vertices, edges, and faces). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fef030",
   "metadata": {},
   "source": [
    "### Batching\n",
    "\n",
    "In simulations, we may want to \"batch\" over several initial conditions/random seeds/etc (analogous to batching over training data in normal ML). In JAX, one can efficiently and concisely vectorize operations over such \"batch axes\" with `jax.vmap`. \n",
    "\n",
    "To batch over our custom data structures, we need to pull a small trick - convert a list of instances into a single mesh with a batch axis for the various arrays. Luckily, this can be [done using JAX's pytree tools](https://stackoverflow.com/questions/79123001/storing-and-jax-vmap-over-pytrees)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea78f52",
   "metadata": {},
   "source": [
    "## Simulation loops with `jax.lax.scan`\n",
    "\n",
    "In simulations, we generally start with an initial state (call it `init`), do a series of timesteps (via a function `make_step(state)`), and record some \"measurement\" at each timestep (via a `measure(state)` function). As a result, we get a timeseries of measurements, and the final simulation state. In normal python, you would do that with a `for` loop. When working with JAX, we need to [replace control-flow operations like `for` with their JAX pendant](https://docs.jax.dev/en/latest/control-flow.html). For for loops, this is `jax.lax.scan(f, init, xs)`, which is equivalent to the python code\n",
    "```python\n",
    "def scan(f, init, xs):\n",
    "  carry = init\n",
    "  ys = []\n",
    "  for x in xs:\n",
    "    carry, y = f(carry, x)\n",
    "    ys.append(y)\n",
    "  return carry, np.stack(ys)\n",
    "```\n",
    "In our pattern, `xs` is the vector of time-points `timepoints`, and the \"scanning-function\" `f` is generally comprised of two parts, a time-step and a measurement/logging step (above, we logged energy and T1 count):\n",
    "```python\n",
    "def f(carry, t):\n",
    "    new_state = make_step(carry, t)\n",
    "    measurements = measure(new_state)\n",
    "    return new_state, measurements\n",
    "```\n",
    "\n",
    "The `carry` variable contains all information about the state of the simulation. Typically, `carry` is also composed of multiple pieces (the, the physical state `physical_state`, as well as ancilliary variables like the ODE solver state `solver_state`). To keep things organized, it can make sense to define dataclasses for the simulation state and the measurements, like this (schematic) example:\n",
    "\n",
    "```python\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class SimState:\n",
    "    physical_state: jax.Array\n",
    "    solver_state: dict  # or another PyTree\n",
    "    current_time: jax.Array\n",
    "\n",
    "@jax.tree_util.register_dataclass\n",
    "@dataclass\n",
    "class Log:\n",
    "    energy: float\n",
    "\n",
    "def scan_function(carry: SimState, next_time: jax.Array) -> tuple[SimState, Log]:\n",
    "    physical_state, solver_state = make_step(carry.physical_state, carry.solver_state,\n",
    "                                             carry.current_time, next_time)\n",
    "    log = Log(energy=compute_energy(physical_state))\n",
    "    return SimState(physical_state, solver_state), log\n",
    "\n",
    "timepoints = jnp.arange(t0, t1, dt)\n",
    "init = ... # define initial condition\n",
    "final_state, measurements = jax.lax.scan(scan_function, init, timepoints) \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triangulax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
